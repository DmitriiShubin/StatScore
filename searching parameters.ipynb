{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features when aggregated by mean. Hopefully this shows how you can incorporate more categorical data into your kernel.\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "import warnings\n",
    "import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "print(os.listdir(\"input\"))\n",
    "import sklearn\n",
    "np.random.seed(1)\n",
    "from bayes_opt import BayesianOptimization\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import gc\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom libraries\n",
    "from memory_usage import *\n",
    "from mean_encoding import *\n",
    "from Preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Train and Test Data\n",
    "df_train = pd.read_csv(\"input/train_processed.csv\",index_col=0)\n",
    "df_test = pd.read_csv(\"input/test_processed.csv\",index_col=0)\n",
    "\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicated columns\n",
    "df_test = drop_same_col(df_test)\n",
    "df_train = drop_same_col(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_number = 33.21875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target scaling\n",
    "def targ_descal(a,scaling_number):\n",
    "    return a*scaling_number\n",
    "    #return (np.exp(a)-2)*scaling_number\n",
    "    #return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for getting embeddings from meta-data\n",
    "def aggregate_num(df,df_new,num):\n",
    "    \n",
    "    \n",
    "    #numerical features processing\n",
    "    for i in num:\n",
    "        #mean\n",
    "        data = df_new.groupby('card_id')[i].mean()\n",
    "        columns = df_new.groupby('card_id')[i].mean().index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_mean'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        \n",
    "        #sum\n",
    "        data = df_new.groupby('card_id')[i].sum()\n",
    "        columns = df_new.groupby('card_id')[i].sum().index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_sum'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "\n",
    "        #max\n",
    "        data = df_new.groupby('card_id')[i].max()\n",
    "        columns = df_new.groupby('card_id')[i].max().index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_max'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        \n",
    "        #min\n",
    "        data = df_new.groupby('card_id')[i].min()\n",
    "        columns = df_new.groupby('card_id')[i].min().index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_min'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        \n",
    "        #median\n",
    "        data = df_new.groupby('card_id')[i].median()\n",
    "        columns = df_new.groupby('card_id')[i].median().index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_median'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        \n",
    "        #std\n",
    "        data = df_new.groupby('card_id')[i].std()\n",
    "        columns = df_new.groupby('card_id')[i].std().index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_std'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        \n",
    "\n",
    "        \n",
    "    del buf,data,columns\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main_cat = list(df_train.drop(['target','card_id','elapsed_time_main'],axis=1).columns)\n",
    "#for i in main_cat:\n",
    "    #print(i,df_train[i].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nan-features:\n",
    "\n",
    "df_train['merchant_group_id_merchant_new_mf'][df_train['merchant_group_id_merchant_new_mf'] != 35] = 0\n",
    "df_train['merchant_group_id_merchant_new_mf'][df_train['merchant_group_id_merchant_new_mf'] != 0] = 1\n",
    "df_test['merchant_group_id_merchant_new_mf'][df_test['merchant_group_id_merchant_new_mf'] != 35] = 0\n",
    "df_test['merchant_group_id_merchant_new_mf'][df_test['merchant_group_id_merchant_new_mf'] != 0] = 1\n",
    "\n",
    "\n",
    "df_train['merchant_group_id_merchant_hist_mf'][df_train['merchant_group_id_merchant_hist_mf'] != 35] = 0\n",
    "df_train['merchant_group_id_merchant_hist_mf'][df_train['merchant_group_id_merchant_hist_mf'] != 0] = 1\n",
    "df_test['merchant_group_id_merchant_hist_mf'][df_test['merchant_group_id_merchant_hist_mf'] != 35] = 0\n",
    "df_test['merchant_group_id_merchant_hist_mf'][df_test['merchant_group_id_merchant_hist_mf'] != 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_na = df_test.columns[df_test.isnull().any()].tolist()\n",
    "train_na = df_train.columns[df_train.isnull().any()].tolist()\n",
    "na_cat = list(set(test_na).intersection(train_na))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in na_cat:\n",
    "    df_train['{}_n'.format(i)] = 0\n",
    "    df_train['{}_n'.format(i)][df_train[i].isnull()] = 1\n",
    "\n",
    "for i in na_cat:\n",
    "    df_test['{}_n'.format(i)] = 0\n",
    "    df_test['{}_n'.format(i)][df_test[i].isnull()] = 1\n",
    "    \n",
    "del na_cat,test_na,train_na\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_cat = list(df_train.drop(['target','card_id','elapsed_time_main'],axis=1).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans = pd.read_csv(\"input/historical_transactions.csv\",index_col=0)\n",
    "print('Loading of hist data is finished')\n",
    "\n",
    "df_new_trans = pd.read_csv(\"input/new_merchant_transactions.csv\",index_col=0)\n",
    "print('Loading of new data is finished')\n",
    "\n",
    "merchants = pd.read_csv(\"input/merchants.csv\")\n",
    "print('Loading of data is finished')\n",
    "\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)\n",
    "df_new_trans = reduce_mem_usage(df_new_trans)\n",
    "merchants = reduce_mem_usage(merchants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans = drop_same_col(df_hist_trans)\n",
    "df_new_trans = drop_same_col(df_new_trans)\n",
    "merchants = drop_same_col(merchants)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans = merged_data_processing(df_hist_trans)\n",
    "df_new_trans = merged_data_processing(df_new_trans)\n",
    "merchants = merchant_proc(merchants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist_trans = pd.merge(df_hist_trans,merchants,on='merchant_id',how='left')\n",
    "df_new_trans = pd.merge(df_new_trans,merchants,on='merchant_id',how='left')\n",
    "\n",
    "df_new_trans['merchant_id_n'] = 0\n",
    "df_new_trans['merchant_id_n'][df_new_trans['merchant_id'].isnull()] =1\n",
    "df_new_trans = rename_col(df_new_trans,'new')\n",
    "df_new_trans.head()\n",
    "\n",
    "df_hist_trans['merchant_id_n'] = 0\n",
    "df_hist_trans['merchant_id_n'][df_hist_trans['merchant_id'].isnull()] =1\n",
    "df_hist_trans = rename_col(df_hist_trans,'hist')\n",
    "df_hist_trans.head()\n",
    "\n",
    "targ = df_train[['card_id','target']].copy()\n",
    "\n",
    "df_new_trans = pd.merge(df_new_trans,targ,on='card_id',how='left')\n",
    "df_hist_trans = pd.merge(df_hist_trans,targ,on='card_id',how='left')\n",
    "\n",
    "del targ,merchants\n",
    "gc.collect()\n",
    "\n",
    "df_new_trans = reduce_mem_usage(df_new_trans)\n",
    "df_hist_trans = reduce_mem_usage(df_hist_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_trans['merchant_group_id_merchant_new'][df_new_trans['merchant_group_id_merchant_new'] != 35] = 0\n",
    "df_new_trans['merchant_group_id_merchant_new'][df_new_trans['merchant_group_id_merchant_new'] != 0] = 1\n",
    "\n",
    "\n",
    "df_hist_trans['merchant_group_id_merchant_hist'][df_hist_trans['merchant_group_id_merchant_hist'] != 35] = 0\n",
    "df_hist_trans['merchant_group_id_merchant_hist'][df_hist_trans['merchant_group_id_merchant_hist'] != 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_num = ['purchase_amount_hist','numerical_1_merchant_hist','numerical_2_merchant_hist','avg_sales_lag3_merchant_hist',\n",
    "          'avg_purchases_lag3_merchant_hist','avg_sales_lag6_merchant_hist','avg_purchases_lag6_merchant_hist',\n",
    "          'avg_sales_lag12_merchant_hist','avg_purchases_lag12_merchant_hist']\n",
    "buf = df_hist_trans.drop(['card_id','merchant_id','target'],axis=1)\n",
    "hist_cat_enc = list(buf.drop(hist_num,axis=1).columns)\n",
    "del buf\n",
    "gc.collect()\n",
    "\n",
    "new_num = ['purchase_amount_new','numerical_1_merchant_new','numerical_2_merchant_new','avg_sales_lag3_merchant_new',\n",
    "          'avg_purchases_lag3_merchant_new','avg_sales_lag6_merchant_new','avg_purchases_lag6_merchant_new',\n",
    "          'avg_sales_lag12_merchant_new','avg_purchases_lag12_merchant_new']\n",
    "buf = df_new_trans.drop(['card_id','merchant_id','target'],axis=1)\n",
    "new_cat_enc = list(buf.drop(new_num,axis=1).columns)\n",
    "del buf\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in new_cat_enc:\n",
    "    print(i,df_new_trans[i].unique().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat_enc,new_cat = divide_cat(df_new_trans,new_cat_enc)\n",
    "hist_cat_enc,hist_cat = divide_cat(df_hist_trans,hist_cat_enc)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_hist_trans['card_id'].value_counts()\n",
    "columns = data.index\n",
    "buf = pd.DataFrame({'card_id': columns.values, 'card_id_n_trans_hist': data.values})\n",
    "\n",
    "df_train = pd.merge(df_train,buf,on='card_id',how='left')\n",
    "df_test = pd.merge(df_test,buf,on='card_id',how='left')\n",
    "\n",
    "\n",
    "\n",
    "data = df_new_trans['card_id'].value_counts()\n",
    "columns = data.index\n",
    "buf = pd.DataFrame({'card_id': columns.values, 'card_id_n_trans_new': data.values})\n",
    "\n",
    "df_train = pd.merge(df_train,buf,on='card_id',how='left')\n",
    "df_test = pd.merge(df_test,buf,on='card_id',how='left')\n",
    "\n",
    "df_test['card_id_n_trans_new'] = df_test['card_id_n_trans_new'].fillna(0)\n",
    "df_test['card_id_n_trans_hist'] = df_test['card_id_n_trans_hist'].fillna(0)\n",
    "\n",
    "df_train['card_id_n_trans_new'] = df_train['card_id_n_trans_new'].fillna(0)\n",
    "df_train['card_id_n_trans_hist'] = df_train['card_id_n_trans_hist'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_new = create_alpha(new_cat_enc)\n",
    "alpha_hist = create_alpha(hist_cat_enc)\n",
    "alpha_main = create_alpha(main_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_num(df,df_new,num):\n",
    "    \n",
    "    \n",
    "    #numerical features processing\n",
    "    for i in num:\n",
    "        #print(i)\n",
    "        \n",
    "        #mean\n",
    "        data = df_new.groupby('card_id')[i].mean()\n",
    "        columns = data.index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_mean'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        df['{}_mean'.format(i)] = df['{}_mean'.format(i)].fillna(0)\n",
    "        \n",
    "        #sum\n",
    "        data = df_new.groupby('card_id')[i].sum()\n",
    "        columns = data.index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_sum'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        df['{}_sum'.format(i)] = df['{}_sum'.format(i)].fillna(0)\n",
    "\n",
    "        #max\n",
    "        data = df_new.groupby('card_id')[i].max()\n",
    "        columns = data.index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_max'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        df['{}_max'.format(i)] = df['{}_max'.format(i)].fillna(0)\n",
    "        \n",
    "        #min\n",
    "        data = df_new.groupby('card_id')[i].min()\n",
    "        columns = data.index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_min'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        df['{}_min'.format(i)] = df['{}_min'.format(i)].fillna(0)\n",
    "        \n",
    "        #median\n",
    "        data = df_new.groupby('card_id')[i].median()\n",
    "        columns = data.index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_median'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        df['{}_median'.format(i)] = df['{}_median'.format(i)].fillna(0)\n",
    "        \n",
    "        #std\n",
    "        data = df_new.groupby('card_id')[i].std()\n",
    "        columns = data.index\n",
    "        buf = pd.DataFrame({'card_id': columns.values, '{}_std'.format(i): data.values})\n",
    "        df = pd.merge(df,buf,on='card_id',how='left')\n",
    "        df['{}_std'.format(i)] = df['{}_std'.format(i)].fillna(0)\n",
    "\n",
    "        \n",
    "    del buf,data,columns\n",
    "    gc.collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_num += new_cat_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'num_leaves': int(round(53.17295519924188)),\n",
    "             'min_data_in_leaf': int(round(47.28877693542961)),\n",
    "             'feature_fraction' : max(min(0.5153314260276387, 1), 0),\n",
    "             'max_depth': int(round(9.958894298585333)),\n",
    "             \"bagging_freq\": int(round(80.11669785745563)),\n",
    "             \"bagging_fraction\": round(min((max(0.9807565080094875,0),1)),1),\n",
    "             \"lambda_l1\": round(min((max(0.7805291762864555,0),1)),1),\n",
    "             \"lambda_l2\": round(min((max(0.11827442586893322,0),1)),1),\n",
    "             \"min_split_gain\":  0.0526629838532571,\n",
    "             \"min_child_weight\": 8.024311083043273,\n",
    "             'objective':'regression',\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"verbosity\": -1,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'rmse',\n",
    "             'learning_rate': 0.005,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_colum = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_calc(score,num_seeds,n_splits,n_boost):\n",
    "    \n",
    "    score = 1/score;\n",
    "\n",
    "    novel = np.zeros(shape=(n_boost,num_seeds*n_splits))\n",
    "\n",
    "    for r in range(n_boost):\n",
    "        for t in range(num_seeds*n_splits):\n",
    "            novel[r,t] = score[randint(0, t)]\n",
    "\n",
    "    m = np.zeros(shape=(n_boost,1))\n",
    "\n",
    "    for r in range(n_boost):\n",
    "        m[r] = np.mean(novel[r,:])\n",
    "\n",
    "    val_precision = (np.mean(m))\n",
    "    val_recall = 1/(np.percentile(m,99.7) - np.percentile(m,0.3))\n",
    "\n",
    "    return ((val_precision*val_recall)/(val_recall+val_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na(df,glob_mean):\n",
    "    df = df.fillna(glob_mean)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from random import randint\n",
    "\n",
    "def lgb_eval(num_leaves, min_data_in_leaf, max_depth, feature_fraction, bagging_freq, bagging_fraction, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n",
    "    param = {'num_leaves': int(round(num_leaves)),\n",
    "             'min_data_in_leaf': int(round(min_data_in_leaf)),\n",
    "             'feature_fraction' : max(min(feature_fraction, 1), 0),\n",
    "             'max_depth': int(round(max_depth)),\n",
    "             \"bagging_freq\": int(round(bagging_freq)),\n",
    "             \"bagging_fraction\": round(min((max(bagging_fraction,0),1)),1),\n",
    "             \"lambda_l1\": round(min((max(lambda_l1,0),1)),1),\n",
    "             \"lambda_l2\": round(min((max(lambda_l2,0),1)),1),\n",
    "             \"min_split_gain\": min_split_gain,\n",
    "             \"min_child_weight\": min_child_weight,\n",
    "             'objective':'regression',\n",
    "             \"boosting\": \"gbdt\",\n",
    "             \"verbosity\": -1,\n",
    "             \"bagging_seed\": 11,\n",
    "             \"metric\": 'rmse',\n",
    "             'learning_rate': 0.005,\n",
    "            }\n",
    "\n",
    "    num_seeds = 1 #num seeds\n",
    "    n_splits = 5 #n folds\n",
    "    n_boost = 100000 #n of samples for bootstrapping\n",
    "\n",
    "    droplist=['card_id','target']\n",
    "\n",
    "    val_score = np.zeros(shape=(num_seeds*n_splits,1))\n",
    "    for sed in range(num_seeds):\n",
    "        np.random.seed(sed)\n",
    "        folds = KFold(n_splits=n_splits, shuffle=True, random_state=sed)\n",
    "        oof = np.zeros(len(df_train))\n",
    "        predictions = np.zeros(len(df_test))\n",
    "\n",
    "        for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train.values)):\n",
    "            print('-')\n",
    "            print(\"Fold {}\".format(fold_ + 1))\n",
    "            print('Seed {}'.format(sed + 1))\n",
    "\n",
    "            train = df_train.iloc[trn_idx]\n",
    "            target = df_train[target_colum].copy()\n",
    "            val = df_train.iloc[val_idx]\n",
    "\n",
    "\n",
    "\n",
    "            train_new = df_new_trans[df_new_trans['card_id'].isin(train['card_id'])]\n",
    "            val_new = df_new_trans[df_new_trans['card_id'].isin(val['card_id'])]\n",
    "\n",
    "            train_new = train_enc(train_new,new_cat_enc,target_colum,alpha_new)\n",
    "            val_new,gb_new,dict_new = test_enc(train_new,val_new,new_cat_enc,target_colum,alpha_new)\n",
    "\n",
    "            train = aggregate_num(train,train_new,new_num)\n",
    "            val = aggregate_num(val,val_new,new_num)\n",
    "\n",
    "\n",
    "            #applying mean target encoding\n",
    "            train = train_enc(train,main_cat,target_colum,alpha_main)\n",
    "            val,gb_main,dict_main = test_enc(train,val,main_cat,target_colum,alpha_main)\n",
    "\n",
    "\n",
    "\n",
    "            #df_test_pred = pred_enc(df_test,dict_main,main_cat)\n",
    "\n",
    "            train = fill_na(train,gb_main)\n",
    "            val = fill_na(val,gb_main)\n",
    "\n",
    "            val = val.drop(droplist,axis=1)\n",
    "            train = train.drop(droplist,axis=1)\n",
    "\n",
    "            trn_data = lgb.Dataset(train, label=target.iloc[trn_idx])\n",
    "            val_data = lgb.Dataset(val, label=target.iloc[val_idx])\n",
    "\n",
    "            \n",
    "\n",
    "            num_round = 1000000\n",
    "            \n",
    "            clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=None, early_stopping_rounds=1000)\n",
    "            \n",
    "            oof[val_idx] = targ_descal(clf.predict(val, num_iteration=clf.best_iteration),scaling_number)\n",
    "            val_score[fold_+sed*n_splits] = np.sqrt(mean_squared_error(targ_descal(target.iloc[val_idx],scaling_number), oof[val_idx]))\n",
    "            #predictions += num_target_descaling(clf.predict(df_test_pred, num_iteration=clf.best_iteration),scaling_number) / n_splits*num_seeds\n",
    "    \n",
    "    \n",
    "    return score_calc(val_score,num_seeds,n_splits,n_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bayesian hyperopt\n",
    "lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (20, 300),\n",
    "                                        'min_data_in_leaf':(1,100),\n",
    "                                        'max_depth': (1,18),\n",
    "                                        'feature_fraction': (0.1, 1),\n",
    "                                        'bagging_fraction': (0.1, 1),\n",
    "                                        'bagging_freq': (1, 100),\n",
    "                                        'lambda_l1': (5, 10),\n",
    "                                        'lambda_l2': (5, 10),\n",
    "                                        'min_split_gain': (0.001, 0.1),\n",
    "                                        'min_child_weight': (1, 50)}, random_state=0)\n",
    "init_round = 10\n",
    "opt_round = 10\n",
    "lgbBO.maximize(init_points=init_round, n_iter=opt_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "убрать отображение фичей\n",
    "сделать скейлинг\n",
    "сделать сохранение параметров\n",
    "уменьшить количество валидационных фолдов до 3\n",
    "уменьшить количество фолдов для mean mean_encoding до 3\n",
    "посмотреть как можно поменять loss в lightgbm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
